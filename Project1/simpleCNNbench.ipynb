{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c35beb2a-037b-439c-a149-52268ff26a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from PIL import Image\n",
    "import os\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "import random\n",
    "import copy\n",
    "\n",
    "import cv2\n",
    "\n",
    "\n",
    "# Define device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "# Define the Hotdog_NotHotdog dataset class\n",
    "class Hotdog_NotHotdog(torch.utils.data.Dataset):\n",
    "    def __init__(self, train, transform, data_path='C:/Users/nicla/DTU/ComputerVision/1st Poster/hotdog_nothotdog'):\n",
    "        'Initialization'\n",
    "        self.transform = transform\n",
    "        data_path = os.path.join(data_path, 'train' if train else 'test')\n",
    "        image_classes = [os.path.split(d)[1] for d in glob.glob(data_path + '/*') if os.path.isdir(d)]\n",
    "        image_classes.sort()\n",
    "        self.name_to_label = {c: id for id, c in enumerate(image_classes)}\n",
    "        self.image_paths = glob.glob(data_path + '/*/*.jpg')\n",
    "        \n",
    "        self.images = [self.load_image(path) for path in self.image_paths]\n",
    "        \n",
    "    def __len__(self):\n",
    "        'Returns the total number of samples'\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def load_image(self,path):\n",
    "        'Generates one sample of data'\n",
    "        image_path = path\n",
    "        image = Image.open(image_path)\n",
    "        c = os.path.split(os.path.split(image_path)[0])[1]\n",
    "        y = self.name_to_label[c]\n",
    "        X = self.transform(image)\n",
    "        return [X, y]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        'Generates one sample of data'\n",
    "        \n",
    "        return self.images[idx][0], self.images[idx][1]\n",
    "\n",
    "\n",
    "# Early stopping utility\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=10, delta=0):\n",
    "        self.patience = patience\n",
    "        self.delta = delta\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.counter = 0\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        score = -val_loss\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.counter = 0\n",
    "\n",
    "\n",
    "# Function to calculate accuracy\n",
    "def calculate_accuracy(loader, model, device):\n",
    "    model.eval()  \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad(): \n",
    "        for inputs, labels in loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            predicted = (outputs > 0.5).float() \n",
    "            correct += (predicted.squeeze() == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy\n",
    "\n",
    "# Training function with early stopping\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs, patience, device):\n",
    "    model.to(device)  # Move model to the specified device\n",
    "    train_accuracies = []\n",
    "    val_accuracies = []\n",
    "    \n",
    "    early_stopping = EarlyStopping(patience=patience)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "\n",
    "        for i, data in enumerate(progress_bar):\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device,non_blocking=False), labels.to(device, non_blocking=False)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs.squeeze(), labels.float())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            progress_bar.set_postfix({'Loss': running_loss / (i + 1)})\n",
    "\n",
    "        # Calculate accuracy on training and validation datasets\n",
    "        train_acc = calculate_accuracy(train_loader, model, device)\n",
    "        val_acc = calculate_accuracy(val_loader, model, device)\n",
    "        train_accuracies.append(train_acc)\n",
    "        val_accuracies.append(val_acc)\n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}, '\n",
    "              f'Train Accuracy: {train_acc:.2f}%, Validation Accuracy: {val_acc:.2f}%')\n",
    "\n",
    "        # Early stopping check\n",
    "        early_stopping(val_acc)\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "\n",
    "    return train_accuracies, val_accuracies\n",
    "\n",
    "# Main\n",
    "def run_experiment(augmentation_methods,filename,input_model, num_runs=5,size=128):\n",
    "    size = size\n",
    "    batch_size = 64\n",
    "    num_epochs = 50\n",
    "    patience = 10\n",
    "    results = []\n",
    "\n",
    "    base_transform = transforms.Compose([\n",
    "        transforms.Resize((size, size)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "    # Define augmentation methods\n",
    "    color_jitter = transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1)\n",
    "    random_rotation = transforms.RandomRotation(degrees=15)\n",
    "    random_horizontal_flip = transforms.RandomHorizontalFlip(p=0.5)\n",
    "    random_affine = transforms.RandomAffine(degrees=15, translate=(0.1, 0.1), scale=(0.8, 1.2))\n",
    "    random_resized_crop = transforms.RandomResizedCrop(size, scale=(0.8, 1.0))\n",
    "\n",
    "    augmentation_dict = {\n",
    "        'color_jitter': color_jitter,\n",
    "        'random_rotation': random_rotation,\n",
    "        'random_horizontal_flip': random_horizontal_flip,\n",
    "        'random_affine': random_affine,\n",
    "        'random_resized_crop': random_resized_crop\n",
    "    }\n",
    "\n",
    "    # Create all combinations of augmentation methods\n",
    "    augmentation_combinations = [\n",
    "        [],\n",
    "        ['color_jitter'],\n",
    "        ['random_rotation'],\n",
    "        ['random_horizontal_flip'],\n",
    "        ['random_affine'],\n",
    "        ['random_resized_crop'],\n",
    "        ['color_jitter', 'random_rotation', 'random_horizontal_flip', 'random_affine', 'random_resized_crop']\n",
    "    ]\n",
    "\n",
    "    for combo in augmentation_combinations:\n",
    "        combo_name = '+'.join(combo) if combo else 'no_augmentation'\n",
    "        print(f\"\\nRunning experiment with {combo_name}\")\n",
    "\n",
    "        for run in range(num_runs):\n",
    "            print(f\"\\nRun {run + 1}/{num_runs}\")\n",
    "\n",
    "            # Create the transform for this combination\n",
    "            train_transform = transforms.Compose([\n",
    "                transforms.Resize((size, size)),\n",
    "                *[augmentation_dict[aug] for aug in combo],\n",
    "                transforms.ToTensor()\n",
    "            ])\n",
    "\n",
    "            # Load and split the datasets\n",
    "            trainset = Hotdog_NotHotdog(train=True, transform=train_transform, data_path=\"../hotdog_nothotdog\")\n",
    "            train_size = int(0.8 * len(trainset))\n",
    "            val_size = len(trainset) - train_size\n",
    "            train_dataset, val_dataset = random_split(trainset, [train_size, val_size])\n",
    "\n",
    "            train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0,pin_memory=True)\n",
    "            val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0,pin_memory=True)\n",
    "\n",
    "            testset = Hotdog_NotHotdog(train=False, transform=base_transform, data_path=\"../hotdog_nothotdog\")\n",
    "            test_loader = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=0,pin_memory=True)\n",
    "\n",
    "            # Initialize the model, loss function, and optimizer\n",
    "            model = copy.deepcopy(input_model)\n",
    "            criterion = nn.BCELoss()\n",
    "            optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "            # Train the model with early stopping\n",
    "            train_accuracies, val_accuracies = train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs, patience, device)\n",
    "\n",
    "            # Calculate test accuracy\n",
    "            test_accuracy = calculate_accuracy(test_loader, model, device)\n",
    "\n",
    "            # Save results\n",
    "            results.append({\n",
    "                'augmentation': combo_name,\n",
    "                'run': run + 1,\n",
    "                'train_accuracy': train_accuracies[-1],\n",
    "                'val_accuracy': val_accuracies[-1],\n",
    "                'test_accuracy': test_accuracy\n",
    "            })\n",
    "\n",
    "    # Save results to CSV\n",
    "    with open(filename, 'w', newline='') as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=['augmentation', 'run', 'train_accuracy', 'val_accuracy', 'test_accuracy'])\n",
    "        writer.writeheader()\n",
    "        for result in results:\n",
    "            writer.writerow(result)\n",
    "\n",
    "    print(\"\\nExperiment completed. Results saved to \", filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "48ee4bfa-c239-467f-9989-12aedf7aef58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the SimpleCNN model\n",
    "_use_batchnorm = True\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(32) if _use_batchnorm else None\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(64) if _use_batchnorm else None\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(64) if _use_batchnorm else None\n",
    "        \n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(64 * 16 * 16, 64)\n",
    "        self.bn_fc1 = nn.BatchNorm1d(64) if _use_batchnorm else None\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(64, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        if _use_batchnorm:\n",
    "            x = self.bn1(x)\n",
    "        x = self.pool(self.relu(x))\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        if _use_batchnorm:\n",
    "            x = self.bn2(x)\n",
    "        x = self.pool(self.relu(x))\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "        if _use_batchnorm:\n",
    "            x = self.bn3(x)\n",
    "        x = self.pool(self.relu(x))\n",
    "        \n",
    "        x = x.view(-1, 64 * 16 * 16)\n",
    "        x = self.fc1(x)\n",
    "        if _use_batchnorm:\n",
    "            x = self.bn_fc1(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        return torch.sigmoid(x)\n",
    "\n",
    "class SimpleCNN4(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN4, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(32) if _use_batchnorm else None\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(64) if _use_batchnorm else None\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(128) if _use_batchnorm else None\n",
    "\n",
    "        self.conv4 = nn.Conv2d(128, 128, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn4 = nn.BatchNorm2d(128) if _use_batchnorm else None\n",
    "        \n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(32 * 16 * 16, 64)\n",
    "        self.bn_fc1 = nn.BatchNorm1d(64) if _use_batchnorm else None\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.pool(self.relu(x))\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.pool(self.relu(x))\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.pool(self.relu(x))\n",
    "\n",
    "        x = self.conv4(x)\n",
    "        x = self.bn4(x)\n",
    "        x = self.pool(self.relu(x))\n",
    "        \n",
    "        x = x.view(-1, 32 * 16 * 16)\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn_fc1(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        return torch.sigmoid(x)\n",
    "\n",
    "class SimpleCNN5(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN5, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(32) if _use_batchnorm else None\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(64) if _use_batchnorm else None\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(128) if _use_batchnorm else None\n",
    "\n",
    "        self.conv4 = nn.Conv2d(128, 256, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn4 = nn.BatchNorm2d(256) if _use_batchnorm else None\n",
    "\n",
    "        self.conv5 = nn.Conv2d(256, 256, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn5 = nn.BatchNorm2d(256) if _use_batchnorm else None\n",
    "        \n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(16 * 16 * 16, 64)\n",
    "        self.bn_fc1 = nn.BatchNorm1d(64) if _use_batchnorm else None\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.pool(self.relu(x))\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.pool(self.relu(x))\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.pool(self.relu(x))\n",
    "\n",
    "        x = self.conv4(x)\n",
    "        x = self.bn4(x)\n",
    "        x = self.pool(self.relu(x))\n",
    "\n",
    "        x = self.conv5(x)\n",
    "        x = self.bn5(x)\n",
    "        x = self.pool(self.relu(x))\n",
    "        \n",
    "        x = x.view(-1, 16 * 16 * 16)\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn_fc1(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        return torch.sigmoid(x)\n",
    "\n",
    "class SimpleCNN6(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN6, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(32) if _use_batchnorm else None\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(64) if _use_batchnorm else None\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(128) if _use_batchnorm else None\n",
    "\n",
    "        self.conv4 = nn.Conv2d(128, 256, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn4 = nn.BatchNorm2d(256) if _use_batchnorm else None\n",
    "\n",
    "        self.conv5 = nn.Conv2d(256, 512, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn5 = nn.BatchNorm2d(512) if _use_batchnorm else None\n",
    "\n",
    "        self.conv6 = nn.Conv2d(512, 512, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn6 = nn.BatchNorm2d(512) if _use_batchnorm else None\n",
    "        \n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(8 * 16 * 16, 64)\n",
    "        self.bn_fc1 = nn.BatchNorm1d(64) if _use_batchnorm else None\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.pool(self.relu(x))\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.pool(self.relu(x))\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.pool(self.relu(x))\n",
    "\n",
    "        x = self.conv4(x)\n",
    "        x = self.bn4(x)\n",
    "        x = self.pool(self.relu(x))\n",
    "\n",
    "        x = self.conv5(x)\n",
    "        x = self.bn5(x)\n",
    "        x = self.pool(self.relu(x))\n",
    "\n",
    "        x = self.conv6(x)\n",
    "        x = self.bn6(x)\n",
    "        x = self.pool(self.relu(x))\n",
    "        \n",
    "        x = x.view(-1, 8 * 16 * 16)\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn_fc1(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        return torch.sigmoid(x)\n",
    "\n",
    "\n",
    "\n",
    "class SimpleCNN_custominit(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN_custominit, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(32) if _use_batchnorm else None\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(64) if _use_batchnorm else None\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(64) if _use_batchnorm else None\n",
    "        \n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(64 * 16 * 16, 64)\n",
    "        self.bn_fc1 = nn.BatchNorm1d(64) if _use_batchnorm else None\n",
    "        \n",
    "        self.fc2 = nn.Linear(64, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self._initialize_weights()\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        if _use_batchnorm:\n",
    "            x = self.bn1(x)\n",
    "        x = self.pool(self.relu(x))\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        if _use_batchnorm:\n",
    "            x = self.bn2(x)\n",
    "        x = self.pool(self.relu(x))\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "        if _use_batchnorm:\n",
    "            x = self.bn3(x)\n",
    "        x = self.pool(self.relu(x))\n",
    "        \n",
    "        x = x.view(-1, 64 * 16 * 16)\n",
    "        x = self.fc1(x)\n",
    "        if _use_batchnorm:\n",
    "            x = self.bn_fc1(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        return torch.sigmoid(x)\n",
    "    def _initialize_weights(self):\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, nn.Conv2d):\n",
    "                    nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.constant_(m.bias, 0)\n",
    "                elif isinstance(m, nn.Linear):\n",
    "                    nn.init.xavier_normal_(m.weight)\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.constant_(m.bias, 0)\n",
    "\n",
    "class SimpleCNN6_custominit(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN6_custominit, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(32) if _use_batchnorm else None\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(64) if _use_batchnorm else None\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(128) if _use_batchnorm else None\n",
    "\n",
    "        self.conv4 = nn.Conv2d(128, 256, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn4 = nn.BatchNorm2d(256) if _use_batchnorm else None\n",
    "\n",
    "        self.conv5 = nn.Conv2d(256, 512, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn5 = nn.BatchNorm2d(512) if _use_batchnorm else None\n",
    "\n",
    "        self.conv6 = nn.Conv2d(512, 512, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn6 = nn.BatchNorm2d(512) if _use_batchnorm else None\n",
    "        \n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(8 * 16 * 16, 64)\n",
    "        self.bn_fc1 = nn.BatchNorm1d(64) if _use_batchnorm else None\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(64, 1)\n",
    "        self._initialize_weights()\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.pool(self.relu(x))\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.pool(self.relu(x))\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.pool(self.relu(x))\n",
    "\n",
    "        x = self.conv4(x)\n",
    "        x = self.bn4(x)\n",
    "        x = self.pool(self.relu(x))\n",
    "\n",
    "        x = self.conv5(x)\n",
    "        x = self.bn5(x)\n",
    "        x = self.pool(self.relu(x))\n",
    "\n",
    "        x = self.conv6(x)\n",
    "        x = self.bn6(x)\n",
    "        x = self.pool(self.relu(x))\n",
    "        \n",
    "        x = x.view(-1, 8 * 16 * 16)\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn_fc1(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        return torch.sigmoid(x)\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, nn.Conv2d):\n",
    "                    nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.constant_(m.bias, 0)\n",
    "                elif isinstance(m, nn.Linear):\n",
    "                    nn.init.xavier_normal_(m.weight)\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.constant_(m.bias, 0)\n",
    "\n",
    "\n",
    "class SimpleCNN6_big(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN6_big, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(32) if _use_batchnorm else None\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(64) if _use_batchnorm else None\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(128) if _use_batchnorm else None\n",
    "\n",
    "        self.conv4 = nn.Conv2d(128, 256, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn4 = nn.BatchNorm2d(256) if _use_batchnorm else None\n",
    "\n",
    "        self.conv5 = nn.Conv2d(256, 512, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn5 = nn.BatchNorm2d(512) if _use_batchnorm else None\n",
    "\n",
    "        self.conv6 = nn.Conv2d(512, 512, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn6 = nn.BatchNorm2d(512) if _use_batchnorm else None\n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(144*32, 144)\n",
    "        self.bn_fc1 = nn.BatchNorm1d(144) if _use_batchnorm else None\n",
    "        \n",
    "        self.fc2 = nn.Linear(144, 1)\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.pool(self.relu(x))\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.pool(self.relu(x))\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.pool(self.relu(x))\n",
    "\n",
    "        x = self.conv4(x)\n",
    "        x = self.bn4(x)\n",
    "        x = self.pool(self.relu(x))\n",
    "\n",
    "        x = self.conv5(x)\n",
    "        x = self.bn5(x)\n",
    "        x = self.pool(self.relu(x))\n",
    "\n",
    "        x = self.conv6(x)\n",
    "        x = self.bn6(x)\n",
    "        x = self.pool(self.relu(x))\n",
    "        \n",
    "        x = x.view(-1, 144*32)\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn_fc1(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        return torch.sigmoid(x)\n",
    "\n",
    "class SimpleCNN6_dropout(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN6_dropout, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(32) if _use_batchnorm else None\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(64) if _use_batchnorm else None\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(128) if _use_batchnorm else None\n",
    "\n",
    "        self.conv4 = nn.Conv2d(128, 256, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn4 = nn.BatchNorm2d(256) if _use_batchnorm else None\n",
    "\n",
    "        self.conv5 = nn.Conv2d(256, 512, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn5 = nn.BatchNorm2d(512) if _use_batchnorm else None\n",
    "\n",
    "        self.conv6 = nn.Conv2d(512, 512, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn6 = nn.BatchNorm2d(512) if _use_batchnorm else None\n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(8 * 16 * 16, 64)\n",
    "        self.bn_fc1 = nn.BatchNorm1d(64) if _use_batchnorm else None\n",
    "\n",
    "        self.drop = nn.Dropout2d(0.2)\n",
    "        self.dropfc = nn.Dropout(0.4)\n",
    "        \n",
    "        self.fc2 = nn.Linear(64, 1)\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.pool(self.relu(x))\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.pool(self.relu(x))\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.pool(self.drop(self.relu(x)))\n",
    "\n",
    "        x = self.conv4(x)\n",
    "        x = self.bn4(x)\n",
    "        x = self.pool(self.drop(self.relu(x)))\n",
    "\n",
    "        x = self.conv5(x)\n",
    "        x = self.bn5(x)\n",
    "        x = self.pool(self.drop(self.relu(x)))\n",
    "\n",
    "        x = self.conv6(x)\n",
    "        x = self.bn6(x)\n",
    "        x = self.pool(self.drop(self.relu(x)))\n",
    "        \n",
    "        x = x.view(-1, 8 * 16 * 16)\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn_fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropfc(x)\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        return torch.sigmoid(x)\n",
    "\n",
    "\n",
    "class SimpleCNN6All(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN6All, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(32) if _use_batchnorm else None\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(64) if _use_batchnorm else None\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(128) if _use_batchnorm else None\n",
    "\n",
    "        self.conv4 = nn.Conv2d(128, 256, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn4 = nn.BatchNorm2d(256) if _use_batchnorm else None\n",
    "\n",
    "        self.conv5 = nn.Conv2d(256, 512, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn5 = nn.BatchNorm2d(512) if _use_batchnorm else None\n",
    "\n",
    "        self.conv6 = nn.Conv2d(512, 512, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn6 = nn.BatchNorm2d(512) if _use_batchnorm else None\n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(144*32, 64)\n",
    "        self.bn_fc1 = nn.BatchNorm1d(64) if _use_batchnorm else None\n",
    "\n",
    "        self.drop = nn.Dropout2d(0.2)\n",
    "        self.dropfc = nn.Dropout(0.4)\n",
    "        \n",
    "        self.fc2 = nn.Linear(64, 1)\n",
    "\n",
    "        self._initialize_weights()\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.pool(self.relu(x))\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.pool(self.relu(x))\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.pool(self.drop(self.relu(x)))\n",
    "\n",
    "        x = self.conv4(x)\n",
    "        x = self.bn4(x)\n",
    "        x = self.pool(self.drop(self.relu(x)))\n",
    "\n",
    "        x = self.conv5(x)\n",
    "        x = self.bn5(x)\n",
    "        x = self.pool(self.drop(self.relu(x)))\n",
    "\n",
    "        x = self.conv6(x)\n",
    "        x = self.bn6(x)\n",
    "        x = self.pool(self.drop(self.relu(x)))\n",
    "        \n",
    "        x = x.view(-1, 144*32)\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn_fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropfc(x)\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        return torch.sigmoid(x)\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, nn.Conv2d):\n",
    "                    nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.constant_(m.bias, 0)\n",
    "                elif isinstance(m, nn.Linear):\n",
    "                    nn.init.xavier_normal_(m.weight)\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.constant_(m.bias, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a03b69aa-cf41-4aea-a879-00279f68baf2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running experiment with no_augmentation\n",
      "\n",
      "Run 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50:  88%|███████████████████████████████████████████████████████████████████████████████████▏          | 23/26 [00:32<00:04,  1.40s/it, Loss=0.551]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[50], line 6\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Run the experiment\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m#model = SimpleCNN()\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m#run_experiment([],filename=\"simple.csv\",input_model=SimpleCNN(),num_runs=5,size=128)\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m#run_experiment([],filename=\"simple4.csv\",input_model=SimpleCNN4(),num_runs=5,size=128)\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m run_experiment([],filename\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msimple5.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m,input_model\u001b[38;5;241m=\u001b[39mSimpleCNN5(),num_runs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m,size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m)\n\u001b[0;32m      7\u001b[0m run_experiment([],filename\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msimple6.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m,input_model\u001b[38;5;241m=\u001b[39mSimpleCNN6(),num_runs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m,size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m#run_experiment([],filename=\"simple_kaiming.csv\",input_model=SimpleCNN_custominit(),num_runs=5,size=128)\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[28], line 206\u001b[0m, in \u001b[0;36mrun_experiment\u001b[1;34m(augmentation_methods, filename, input_model, num_runs, size)\u001b[0m\n\u001b[0;32m    203\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-4\u001b[39m)\n\u001b[0;32m    205\u001b[0m \u001b[38;5;66;03m# Train the model with early stopping\u001b[39;00m\n\u001b[1;32m--> 206\u001b[0m train_accuracies, val_accuracies \u001b[38;5;241m=\u001b[39m train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs, patience, device)\n\u001b[0;32m    208\u001b[0m \u001b[38;5;66;03m# Calculate test accuracy\u001b[39;00m\n\u001b[0;32m    209\u001b[0m test_accuracy \u001b[38;5;241m=\u001b[39m calculate_accuracy(test_loader, model, device)\n",
      "Cell \u001b[1;32mIn[28], line 111\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, train_loader, val_loader, criterion, optimizer, num_epochs, patience, device)\u001b[0m\n\u001b[0;32m    108\u001b[0m inputs, labels \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m    110\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m--> 111\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(inputs)\n\u001b[0;32m    112\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs\u001b[38;5;241m.\u001b[39msqueeze(), labels\u001b[38;5;241m.\u001b[39mfloat())\n\u001b[0;32m    113\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[48], line 130\u001b[0m, in \u001b[0;36mSimpleCNN5.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    128\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv3(x)\n\u001b[0;32m    129\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn3(x)\n\u001b[1;32m--> 130\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(x))\n\u001b[0;32m    132\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv4(x)\n\u001b[0;32m    133\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn4(x)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:103\u001b[0m, in \u001b[0;36mReLU.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 103\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28minput\u001b[39m, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minplace)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\functional.py:1500\u001b[0m, in \u001b[0;36mrelu\u001b[1;34m(input, inplace)\u001b[0m\n\u001b[0;32m   1498\u001b[0m     result \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu_(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m   1499\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1500\u001b[0m     result \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m   1501\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Run the experiment\n",
    "\n",
    "#model = SimpleCNN()\n",
    "#run_experiment([],filename=\"simple.csv\",input_model=SimpleCNN(),num_runs=5,size=128)\n",
    "#run_experiment([],filename=\"simple4.csv\",input_model=SimpleCNN4(),num_runs=5,size=128)\n",
    "run_experiment([],filename=\"simple5.csv\",input_model=SimpleCNN5(),num_runs=5,size=128)\n",
    "run_experiment([],filename=\"simple6.csv\",input_model=SimpleCNN6(),num_runs=5,size=128)\n",
    "#run_experiment([],filename=\"simple_kaiming.csv\",input_model=SimpleCNN_custominit(),num_runs=5,size=128)\n",
    "run_experiment([],filename=\"simple6_kaiming.csv\",input_model=SimpleCNN6_custominit(),num_runs=5,size=128)\n",
    "run_experiment([],filename=\"simple6_big.csv\",input_model=SimpleCNN6_big(),num_runs=5,size=224)\n",
    "run_experiment([],filename=\"simple6_dropout.csv\",input_model=SimpleCNN6_dropout(),num_runs=5,size=128)\n",
    "run_experiment([],filename=\"simple6_full.csv\",input_model=SimpleCNN6All(),num_runs=5,size=224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6dc7dc-c0dd-42af-b06d-aaecf9a47d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from prettytable import PrettyTable\n",
    "\n",
    "def count_parameters(model):\n",
    "    table = PrettyTable([\"Modules\", \"Parameters\"])\n",
    "    total_params = 0\n",
    "    count = 0\n",
    "    for name,parameter in model.named_parameters():\n",
    "        #if not parameter.requires_grad:\n",
    "           # continue\n",
    "        params = parameter.numel()\n",
    "        table.add_row([name, params])\n",
    "        count += 1\n",
    "        total_params += params\n",
    "    print(table)\n",
    "    print(f\"Total Params: {total_params}\")\n",
    "    return total_params\n",
    "\n",
    "count_parameters(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
